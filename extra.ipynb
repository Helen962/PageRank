{"cells":[{"cell_type":"code","execution_count":1,"id":"7bb94573","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/05/03 13:40:35 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/05/03 13:40:36 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/05/03 13:40:36 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/05/03 13:40:36 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructField\n","from pyspark.sql.types import StructType\n","from pyspark.sql.types import StringType,FloatType\n","spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"code","execution_count":2,"id":"3bb3e3a8","metadata":{},"outputs":[],"source":["struct2 = StructType([StructField(\"_c0\", StringType()),StructField(\"_c1\", FloatType())])"]},{"cell_type":"code","execution_count":3,"id":"4059d6a5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING: Use of this script to execute dfs is deprecated.\n","WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n","\n","WARNING: Use of this script to execute dfs is deprecated.\n","WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n","\n","Found 5 items\n","-rw-r--r--   1 mx2257 hadoop  702821930 2022-05-03 13:03 /enwiki_small.xml\n","drwxr-xr-x   - root   hadoop          0 2022-05-03 13:14 /p2t1\n","drwxrwxrwt   - hdfs   hadoop          0 2022-05-03 13:00 /tmp\n","drwxrwxrwt   - hdfs   hadoop          0 2022-05-03 13:11 /user\n","drwx-wx-wx   - hive   hadoop          0 2022-05-03 13:00 /var\n"]}],"source":["!hadoop dfs -mkdir -p hdfs://extra-m/p2t1/p1t3Out\n","!hadoop dfs -ls /"]},{"cell_type":"code","execution_count":4,"id":"aabe874a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING: Use of this script to execute dfs is deprecated.\n","WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n","\n","WARNING: Use of this script to execute dfs is deprecated.\n","WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n","\n","Found 5 items\n","-rw-r--r--   1 mx2257 hadoop  702821930 2022-05-03 13:03 /enwiki_small.xml\n","drwxr-xr-x   - root   hadoop          0 2022-05-03 13:14 /p2t1\n","drwxrwxrwt   - hdfs   hadoop          0 2022-05-03 13:00 /tmp\n","drwxrwxrwt   - hdfs   hadoop          0 2022-05-03 13:11 /user\n","drwx-wx-wx   - hive   hadoop          0 2022-05-03 13:00 /var\n"]}],"source":["!hadoop dfs -mkdir -p hdfs://extra-m/p2t1/p2t1Out\n","!hadoop dfs -ls /"]},{"cell_type":"markdown","id":"035c2b69","metadata":{},"source":["## #p1t3 running"]},{"cell_type":"code","execution_count":5,"id":"758b108a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df = spark \\\n","    .read.format(\"csv\") \\\n","    .option(\"sep\",\"\\t\") \\\n","    .option(\"header\", \"false\") \\\n","    .option(\"inferSchema\", \"true\") \\\n","    .load(\"gs://systemhw2/task2.csv\") \\\n","    .dropna()"]},{"cell_type":"code","execution_count":6,"id":"15ab42cb","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["rank = df.select([\"_c0\",\"_c1\"]).rdd.flatMap(lambda x : [x[0],x[1]]).distinct().map(lambda x: (x,1))\n","rank_df = rank.toDF().withColumnRenamed(\"_1\",\"rank_link\").withColumnRenamed(\"_2\",\"rank_value\")\n","link = df.select([\"_c1\"]).rdd.flatMap(lambda x : x).distinct().map(lambda x: (x,1)) # link\n","\n","only_title = rank.subtractByKey(link).distinct().map(lambda x: (x[0],0)) # in title not in link\n","\n","count_title = df.rdd.map(lambda x: (x[0],1)).reduceByKey(lambda y,z: y+z).toDF().withColumnRenamed(\"_1\",\"count_link\").withColumnRenamed(\"_2\",\"count_value\") # title's number of neighbours    \n","\n","joined_table = df.join(count_title,df[\"_c0\"] == count_title[\"count_link\"]).select([\"_c0\", \"_c1\",\"count_value\"])"]},{"cell_type":"code","execution_count":7,"id":"ac874882","metadata":{},"outputs":[],"source":["def linkMatching(count_title,joined_table, rank, only_title): \n","    \n","    whole_table = joined_table.join(rank,joined_table[\"_c0\"] == rank[\"rank_link\"]).select([\"_c0\", \"_c1\", \"count_value\",\"rank_value\"])\n","    \n","    rank_table = whole_table.withColumn(\"contribution\", whole_table.rank_value/whole_table.count_value)\n","    \n","    new_rdd = rank_table.select([\"_c1\",\"contribution\"]).rdd.reduceByKey(lambda x,y: x+y).map(lambda y: (y[0],y[1]*0.85+0.15))\n","    \n","    new_table = new_rdd.union(only_title).toDF().fillna(0).withColumnRenamed(\"_1\",\"rank_link\").withColumnRenamed(\"_2\",\"rank_value\")\n","    \n","    return new_table"]},{"cell_type":"code","execution_count":8,"id":"4ec75541","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["rank_new = linkMatching(count_title,joined_table,rank_df, only_title)\n","\n","for i in range(9):\n","    rank_new = linkMatching(count_title,joined_table,rank_new, only_title)"]},{"cell_type":"code","execution_count":9,"id":"844a970b","metadata":{},"outputs":[],"source":["sorted_rank = rank_new.sort([\"rank_link\",\"rank_value\"],ascending=True)"]},{"cell_type":"code","execution_count":null,"id":"5280db3e","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":10,"id":"61d8c5a3","metadata":{},"outputs":[],"source":["task3_df = spark \\\n","    .readStream \\\n","    .schema(struct2) \\\n","    .option(\"sep\", \"\\t\") \\\n","    .csv(\"hdfs://extra-m/p2t1/p1t3Out/\") "]},{"cell_type":"code","execution_count":11,"id":"cc209d87","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/05/03 13:43:22 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"]}],"source":["query = task3_df.writeStream \\\n","    .format(\"csv\") \\\n","    .option(\"checkpointLocation\", \"hdfs://extra-m/p2t1/p2t1Out/\") \\\n","    .option(\"path\", \"hdfs://extra-m/p2t1/p2t1Out/\") \\\n","    .option(\"delimiter\", \"\\t\") \\\n","    .start()"]},{"cell_type":"code","execution_count":12,"id":"7e2f1658","metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["query.isActive"]},{"cell_type":"code","execution_count":13,"id":"d8e3d845","metadata":{},"outputs":[],"source":["hdfs_path = \"hdfs://extra-m/p2t1/p1t3Out\""]},{"cell_type":"code","execution_count":14,"id":"1f47d5f9","metadata":{},"outputs":[],"source":["#sorted_rank=sorted_rank.filter(\"rank_value > 0.5\")"]},{"cell_type":"code","execution_count":15,"id":"bd0f82a0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/05/03 13:43:23 WARN org.apache.spark.util.HadoopFSUtils: The directory hdfs://extra-m/p2t1/p1t3Out was not found. Was it deleted very recently?\n","22/05/03 13:43:23 WARN org.apache.spark.util.HadoopFSUtils: The directory hdfs://extra-m/p2t1/p1t3Out was not found. Was it deleted very recently?\n","                                                                                \r"]}],"source":["sorted_rank.filter(\"rank_value > 0.5\").coalesce(1).write.csv(path=hdfs_path, header=\"false\", mode=\"overwrite\", sep=\"\\t\")"]},{"cell_type":"code","execution_count":16,"id":"059887fa","metadata":{},"outputs":[],"source":["query.stop()"]},{"cell_type":"code","execution_count":17,"id":"c8c9271a","metadata":{},"outputs":[],"source":["ans = spark.read.format('csv') \\\n","    .options(delimiter='\\t') \\\n","    .schema(struct2) \\\n","    .load(\"hdfs://extra-m/p2t1/p2t1Out/\")"]},{"cell_type":"code","execution_count":18,"id":"2f9af89b","metadata":{},"outputs":[],"source":["gcs_bucket = 'systemhw2' \n","gcs_filepath = 'gs://{}/p2t1_count.csv'.format(gcs_bucket)"]},{"cell_type":"code","execution_count":19,"id":"c77fca86","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["ans = [{\"count\":ans.count()}]\n","ans_count = spark.createDataFrame(data=ans, schema = [\"count\"])"]},{"cell_type":"code","execution_count":20,"id":"de5b8672","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["ans_count.write.csv(path=gcs_filepath, header=\"true\", mode=\"overwrite\", sep=\"\\t\")"]},{"cell_type":"code","execution_count":null,"id":"7ebc7a6a","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"aca31fdd","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}